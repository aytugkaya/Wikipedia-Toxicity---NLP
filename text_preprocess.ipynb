{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aytu\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\Aytu\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from contractions import CONTRACTION_MAP #contractions.py file must be in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1070971"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nlp=spacy.load('en_core_web_lg')\n",
    "nlp = spacy.load('en_vectors_web_lg')\n",
    "\n",
    "#en_vectors_web_lg.load()\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "total_vectors #1070971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'[\\,]', ', ', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    #text = re.sub(\"[^a-zA-z0-9'\\s]\", '', txt)\n",
    "    return text\n",
    "\n",
    "def lowercase(doc1):\n",
    "    doc1=nlp(doc1)\n",
    "    no_upper=' '.join([token.text.lower() for token in doc1 ])\n",
    "    return no_upper\n",
    "\n",
    "def tokenize_lemmatize(comment, join=True):\n",
    "    #comment=comment.lower()\n",
    "    doc=nlp(comment)\n",
    "    tokens = [token for token in (doc)]\n",
    "    #tokens = [token.text.lower() for token in tokens]\n",
    "    #tokens=' '.join(tokens)\n",
    "    #lemmas = [token.lemma_ if ((token.pos_!='PRON') & (token.lemma_.isalpha()==True)) for token in tokens]\n",
    "    #lemmas = [token.lemma_ for token in (tokens) if ((token.pos_!='PRON') & (token.lemma_.isalnum()==True)& (token.is_stop==False))]\n",
    "    lemmas = [token.lemma_ for token in (tokens) if ((token.pos_!='PRON') & (token.lemma_.isalnum()==True))] #& (token.is_stop==False))]\n",
    "    if join:\n",
    "        lemmatized=' '.join(lemmas)\n",
    "        return lemmatized\n",
    "    else:\n",
    "        return lemmas\n",
    "\n",
    "def tokenize_lemmatize_stops(comment, join=True):\n",
    "    #comment=comment.lower()\n",
    "    doc=nlp(comment)\n",
    "    tokens = [token for token in (doc)]\n",
    "    #tokens = [token.text.lower() for token in tokens]\n",
    "    #tokens=' '.join(tokens)\n",
    "    #lemmas = [token.lemma_ if ((token.pos_!='PRON') & (token.lemma_.isalpha()==True)) for token in tokens]\n",
    "    #lemmas = [token.lemma_ for token in (tokens) if ((token.pos_!='PRON') & (token.lemma_.isalnum()==True)& (token.is_stop==False))]\n",
    "    lemmas = [token.lemma_ for token in (tokens) if ((token.pos_!='PRON') & (token.lemma_.isalnum()==True) & (token.is_stop==False))]\n",
    "    if join:\n",
    "        lemmatized=' '.join(lemmas)\n",
    "        return lemmatized\n",
    "    else:\n",
    "        return lemmas    \n",
    "\n",
    "def remove_stops (doc):\n",
    "    if type(doc) is str:\n",
    "        doc=nlp(doc)\n",
    " \n",
    "    return (' '.join([token.text for token in doc if token.is_stop==False]))\n",
    "\n",
    "def remove_url (doc):\n",
    "   \n",
    "    return re.sub(r'http*\\S+', '', doc) # remove URLs\n",
    "\n",
    "\n",
    "def join_line(line):\n",
    "    joined=' '.join(line)\n",
    "    return joined\n",
    "\n",
    "def transform_text(self, remove_stops=True):\n",
    "    self=self.apply(lambda x: remove_url(x))\n",
    "    self=self.apply(lambda x: remove_accented_chars(x))\n",
    "    self=self.apply(lambda x: expand_contractions(x,CONTRACTION_MAP))\n",
    "    #remove_accented_chars(text)\n",
    "    self=self.apply(lambda x: remove_special_characters(x))\n",
    "    #expand_contractions(text,CONTRACTION_MAP)\n",
    "    self=self.apply(lambda x: lowercase(x))\n",
    "    if remove_stops:\n",
    "        self=self.apply(lambda x: tokenize_lemmatize_stops(x,join=True))\n",
    "    else:\n",
    "        self=self.apply(lambda x: tokenize_lemmatize(x,join=True))\n",
    "    \n",
    "    return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val=train[125000:]\n",
    "train=train[:125000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"transformed\"]=transform_text(test.comment_text, remove_stops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val[\"transformed\"]=transform_text(train_val.comment_text, remove_stops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"transformed\"]=transform_text(train.comment_text, remove_stops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.threat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_transformed_full.csv')\n",
    "train.to_csv('train_transformed_full.csv')\n",
    "train_val.to_csv('train_val_transformed_full.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
